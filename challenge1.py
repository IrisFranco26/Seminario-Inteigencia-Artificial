# -*- coding: utf-8 -*-
"""Challenge1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Ur5Zh1C4OU0wKVVEqTWFKc5SFoaji3G
"""

# Commented out IPython magic to ensure Python compatibility.
#Iris Nathali Franco Alatorre 
#Seminario Inteligencia Artificial 1
#Gradiente descendiente
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# crear la funcion a optimizar y la mostramos
function = lambda x: x**2 #funcion x^2
x= np.linspace(-1,1,200)  #[-1,1]
plt.plot(x, function(x))
plt.show()

#Crear la funcion de la derivada de x^2 =2x

def deriv(x):
  x_deriv=  2*x 
  return x_deriv

#Creamos la funcion del gradiente recibe 3 parametros

def gradient(x_start, precision, l_r):

  #crear 2  listas donde valores actualizados de x y y son agegados durante cada iteracion x_list y_list
  x_list, y_list =   [x_start], [function(x_start)]

  #se realiza un bucle while obtenemos el pendiente de la derivada

  while True:

     d_x = - deriv(x_start)
    #obtenemos el nuevo valor de x sumanddo el valor previo de x mas la multiplicacó del l_r por la derivada

     x_start += (l_r * d_x)

    #agregamos el nuevo valor de x a la lista  para posteriormente visualizarlo
     x_list.append(x_start)

    #Agregamos el nuevo valor  de y a la lista para posteriormente  visualizarlo
     y_list.append(function(x_start))

    #Salimos del bucle cuando se cumple la condición, saldremos de l bucle cuando la diferencia absoluta entre las dos ya no sea significativa

     if abs(x_list[-1] - x_list[-2]) <= precision:
       break 

  #visualizacion de los resultados

  print("El vaor del minimo local se produce en : " + str(x_start))
  print("Numero de pasos: "+ str(len(x_list))) #pasos o iteraciones para llegar al valor local minimo
  #grafica de dispersion 
  plt.subplot(1,2,2)
  plt.scatter(x_list, y_list,  c="g")
  plt.plot(x_list, y_list, c="g") #seguimiento 
  #graficar  la funcion principal 
  plt.plot(x, function(x), c= "r")
  plt.title("El descenso del gradiente")
  plt.show()
  #presicion sera 0.001

gradient(-1, 0.001, 0.05 ) #punto de partida -1,    learning read 0.05